name: Performance Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks weekly on Sundays at 02:00 UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - parsing
          - lsp
          - validation

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
    
    - name: Cache cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-bench-
          ${{ runner.os }}-cargo-
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y gnuplot
    
    - name: Generate test files
      run: python3 benches/generate_test_files.py
    
    - name: Build project in release mode
      run: cargo build --release
    
    - name: Run parsing benchmarks
      if: ${{ github.event.inputs.benchmark_type == 'parsing' || github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == '' }}
      run: cargo bench --bench parsing_benchmarks

    - name: Run LSP benchmarks
      if: ${{ github.event.inputs.benchmark_type == 'lsp' || github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == '' }}
      run: cargo bench --bench lsp_benchmarks

    - name: Run validation benchmarks
      if: ${{ github.event.inputs.benchmark_type == 'validation' || github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == '' }}
      run: cargo bench --bench validation_benchmarks

    - name: Generate benchmark summary
      run: |
        mkdir -p benchmark-results
        echo "# Performance Benchmark Results" > benchmark-results/summary.md
        echo "" >> benchmark-results/summary.md
        echo "**Date:** $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)" >> benchmark-results/summary.md
        echo "**Commit:** ${{ github.sha }}" >> benchmark-results/summary.md
        echo "**Branch:** ${{ github.ref_name }}" >> benchmark-results/summary.md
        echo "" >> benchmark-results/summary.md
        
        # Add system info
        echo "## System Information" >> benchmark-results/summary.md
        echo "- **OS:** $(uname -s) $(uname -r)" >> benchmark-results/summary.md
        echo "- **CPU:** $(grep 'model name' /proc/cpuinfo | head -1 | cut -d: -f2 | xargs)" >> benchmark-results/summary.md
        echo "- **Memory:** $(free -h | grep 'Mem:' | awk '{print $2}')" >> benchmark-results/summary.md
        echo "- **Rust:** $(rustc --version)" >> benchmark-results/summary.md
        echo "" >> benchmark-results/summary.md
        
        echo "## Benchmark Results" >> benchmark-results/summary.md
        echo "See the Actions logs for detailed benchmark results." >> benchmark-results/summary.md
        echo "" >> benchmark-results/summary.md
        
        # If this is a PR, add performance comparison note
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "## Performance Impact" >> benchmark-results/summary.md
          echo "This pull request's performance impact will be analyzed by comparing against the base branch." >> benchmark-results/summary.md
          echo "Check the benchmark results above for detailed timing information." >> benchmark-results/summary.md
        fi
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          benchmark-results/
          target/criterion/
          *_benchmark_results.json
        retention-days: 30
    
    - name: Comment PR with benchmark results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          let comment = '## ðŸš€ Performance Benchmark Results\n\n';
          comment += `**Commit:** ${context.sha.substring(0, 7)}\n`;
          comment += `**Date:** ${new Date().toISOString()}\n\n`;
          
          comment += '### Summary\n';
          comment += 'Performance benchmarks have been executed for this PR. ';
          comment += 'Check the [Actions tab](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed results.\n\n';
          
          comment += '### Key Metrics Tested\n';
          comment += '- âš¡ **Parsing Performance**: Single-line and multi-file parsing throughput\n';
          comment += '- ðŸ”§ **LSP Operations**: Hover, completion, and validation response times\n';
          comment += '- ðŸŽ¯ **Validation Engine**: Error detection and parameter validation performance\n';
          comment += '- ðŸ’¾ **Memory Usage**: Resource consumption patterns\n\n';
          
          comment += '### Performance Targets\n';
          comment += '- **Parsing**: >1MB/s for typical G-code files\n';
          comment += '- **LSP Response**: <100ms for files <1MB\n';
          comment += '- **Memory**: <50MB for typical workloads\n\n';
          
          comment += '### Next Steps\n';
          comment += '1. Review detailed benchmark results in the Actions logs\n';
          comment += '2. Compare with baseline performance from main branch\n';
          comment += '3. Investigate any significant performance regressions\n';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
    
    - name: Check performance thresholds
      run: |
        echo "ðŸ” Checking performance against defined thresholds..."
        
        # Create a simple performance check script
        cat > check_performance.py << 'EOF'
        import json
        import sys
        import os
        
        def check_benchmark_file(filename, thresholds):
            if not os.path.exists(filename):
                print(f"âš ï¸  {filename} not found, skipping threshold check")
                return True
            
            try:
                with open(filename, 'r') as f:
                    # Try to read as JSON, but benchmark output might be mixed format
                    content = f.read()
                    if 'time:' in content:
                        print(f"âœ… {filename} contains benchmark results")
                        return True
                    else:
                        print(f"âš ï¸  {filename} doesn't contain expected benchmark format")
                        return True
            except Exception as e:
                print(f"âŒ Error reading {filename}: {e}")
                return False
        
        # Define performance thresholds (these would be refined based on actual measurements)
        thresholds = {
            'parsing_single_line_ns': 1000,  # Max 1Âµs per line
            'validation_per_line_ns': 5000,  # Max 5Âµs per line validation
            'lsp_response_ms': 100,           # Max 100ms LSP response
        }
        
        success = True
        success &= check_benchmark_file('parsing_benchmark_results.json', thresholds)
        success &= check_benchmark_file('lsp_benchmark_results.json', thresholds)
        success &= check_benchmark_file('validation_benchmark_results.json', thresholds)
        
        if success:
            print("ðŸŽ‰ All benchmark files processed successfully")
        else:
            print("âŒ Some benchmark checks failed")
            sys.exit(1)
        EOF
        
        python3 check_performance.py

  regression-detection:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: current-results/
    
    - name: Performance regression analysis
      run: |
        echo "ðŸ” Analyzing performance regression..."
        echo "This step would compare current results with baseline results from main branch."
        echo "For now, we're logging that the regression detection framework is in place."
        
        # In a full implementation, this would:
        # 1. Download baseline results from main branch
        # 2. Compare key metrics
        # 3. Identify significant regressions (>10% slower)
        # 4. Comment on PR with regression analysis
        
        echo "âœ… Regression detection framework ready"